=> merge config from configs/resnet18_base.yaml
=> merge config from configs/resnet18_medium_imagenet.yaml
[32m[2023-03-15 22:18:54 resnet18][33m(main.py 239)[39m: INFO Full config saved to output/resnet18/config.yaml
[32m[2023-03-15 22:18:54 resnet18][33m(main.py 242)[39m: INFO AUG:
  COLOR_JITTER: 0.4
  RAND_AUGMENT: rand-m9-mstd0.5-inc1
BASE:
- resnet18_base.yaml
DATA:
  BATCH_SIZE: 32
  DATASET: medium_imagenet
  DATA_PATH: ''
  IMG_SIZE: 32
  INTERPOLATION: bicubic
  NUM_WORKERS: 32
  PIN_MEMORY: true
EVAL_MODE: false
MODEL:
  DROP_RATE: 0.0
  NAME: resnet18
  NUM_CLASSES: 200
  RESNET: {}
  RESUME: ''
OUTPUT: output/resnet18
PRINT_FREQ: 500
SAVE_FREQ: 1
SEED: 0
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  EPOCHS: 10
  LR: 0.0003
  LR_SCHEDULER:
    NAME: cosine
  MIN_LR: 3.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  WARMUP_EPOCHS: 2
  WARMUP_LR: 3.0e-05
[32m[2023-03-15 22:18:54 resnet18][33m(main.py 243)[39m: INFO {"cfg": "configs/resnet18_medium_imagenet.yaml", "opts": null, "batch_size": null, "data_path": null, "resume": null, "output": "output", "eval": false, "throughput": false}
[32m[2023-03-15 22:18:57 resnet18][33m(main.py 73)[39m: INFO N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.
ResNet18(
  #params: 11.28M, #flops: 0.56G
  (conv1): Conv2d(
    3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
    #params: 1.73K, #flops: 1.77M
  )
  (bn1): BatchNorm2d(
    64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
    #params: 0.13K, #flops: 0.33M
  )
  (layer1): Sequential(
    #params: 0.15M, #flops: 0.15G
    (0): ResNetBlock(
Unsupported operator aten::add_ encountered 28 time(s)
Unsupported operator aten::avg_pool2d encountered 1 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layer1.0.shortcut, layer1.1.shortcut, layer2.1.shortcut, layer3.1.shortcut, layer4.1.shortcut













































































































































































































































































































































































































































































































































































































Traceback (most recent call last):
  File "/home/vanessaxteo/sp23-nmep-hw1/main.py", line 245, in <module>
    main(config)
  File "/home/vanessaxteo/sp23-nmep-hw1/main.py", line 94, in main
    train_acc1, train_loss = train_one_epoch(config, model, criterion, data_loader_train, optimizer, epoch)
  File "/home/vanessaxteo/sp23-nmep-hw1/main.py", line 149, in train_one_epoch
    optimizer.step()
  File "/home/vanessaxteo/miniconda3/envs/vision-zoo/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/vanessaxteo/miniconda3/envs/vision-zoo/lib/python3.10/site-packages/torch/optim/optimizer.py", line 113, in wrapper
    return func(*args, **kwargs)
  File "/home/vanessaxteo/miniconda3/envs/vision-zoo/lib/python3.10/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/vanessaxteo/miniconda3/envs/vision-zoo/lib/python3.10/site-packages/torch/optim/adamw.py", line 161, in step
    adamw(params_with_grad,
  File "/home/vanessaxteo/miniconda3/envs/vision-zoo/lib/python3.10/site-packages/torch/optim/adamw.py", line 218, in adamw
    func(params,
  File "/home/vanessaxteo/miniconda3/envs/vision-zoo/lib/python3.10/site-packages/torch/optim/adamw.py", line 267, in _single_tensor_adamw
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
KeyboardInterrupt